{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***Word Embedding***\n",
    "---\n",
    "---\n",
    "---\n",
    "***Word Embedding1***\n",
    "1. *Word Embedding*\n",
    "2. *Word2Vec*\n",
    "3. *Skip-Gram with Negative Sampling, SGNS*\n",
    "4. *GloVe*  \n",
    "5. *FastText*\n",
    "***Word Embedding2*** \n",
    "1. *Pre-trained Word Embedding*\n",
    "2. *Embeddings from Language Model, ELMo*\n",
    "3. *Embedding Visualization*\n",
    "4. *Recommendation System using Document Embedding*\n",
    "5. *Average Word Embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ***Word Embedding***\n",
    "---\n",
    "\n",
    "- 워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***희소 표현(Sparse Representation)***\n",
    "- 앞서 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법\n",
    "- 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고하며 원-핫백터는 희소백터라고 할 수 있음\n",
    "- 희소 표현의 일종인 DTM과 같은 경우에도 특정 문서에 여러 단어가 다수 등장하였으나, 다른 많은 문서에서는 해당 특정 문서에 등장했던 단어들이 전부 등장하지 않는다면 희소성을 가짐\n",
    "- 이러한 희소 벡터들은 공간적인 낭비를 일으킴\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. ***밀집 표현(Dense Representation)***\n",
    "- 희소 표현과 반대되는 표현\n",
    "- 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 됨\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***워드 임베딩(Word Embedding)***\n",
    "- 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 함\n",
    "- 워드 임베딩 방법론으로는 **LSA, Word2Vec, FastText, Glove** 등이 있음\n",
    "\n",
    "\n",
    "-|\t원-핫 벡터|\t임베딩 벡터\n",
    ":---:|---|---\n",
    "**차원**|\t고차원(단어 집합의 크기)|\t저차원\n",
    "**다른 표현**|\t희소 벡터의 일종|\t밀집 벡터의 일종\n",
    "**표현 방법**|\t수동\t|훈련 데이터로부터 학습함\n",
    "**값의 타입**|\t1과 0|\t실수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ***Word2Vec***\n",
    "---\n",
    "\n",
    "- 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 할 수 있는 대표적인 방법\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***희소 표현(Sparse Representation)***\n",
    "\n",
    "- 벡터 또는 행렬의 값 대부분이 0으로 표현되는 방법을 희소 표현이라고 함\n",
    "- **고차원**에 각 차원이 분리된 표현 방법\n",
    "- 희소 벡터는 단어간 유사성을 표현할 수 없는 단점이 존재\n",
    "\n",
    "#### 2. ***분산 표현(Distributed Representation)***\n",
    "- **저차원**에 단어의 의미를 여러 차원에 분산하여 표현\n",
    "- 단어 간 유사도 계산이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3. ***CBOW(Continuous Bag Of Words)***\n",
    "\n",
    "- 주변에 있는 단어들로 중간 단어를 예측하는 방법\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "#### 4. ***Skip-gram***\n",
    "\n",
    "- 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5. ***Word2Vec Traning***\n",
    "1. data load\n",
    "2. data preprocessing\n",
    "3. train word2vec\n",
    "4. model save\n",
    "\n",
    "\n",
    "> #### ***`Word2Vec Module`***\n",
    "***sentences*** : word2vec을 진행할 문장들 <br>\n",
    "***vector_size*** : 사용자가 설정할 벡터 차원 <br>\n",
    "***window*** : window size 설정 (기준단어 앞뒤 단어 수) <br>\n",
    "***min_count*** : 단어 빈도수 제한 <br>\n",
    "***workers*** : 학습 프로세스 수 (동시처리) <br>\n",
    "***sg*** : 0 = 'CBOW' / 1 = 'Skip-gram'\n",
    "\n",
    "\n",
    "- 실습 \n",
    "    - English\n",
    "    - Kokean (예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ntels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x226fdc01408>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 273424\n",
      "샘플 3개만 출력\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 특수 문자 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 문장 토큰화\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 소문자 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 단어 토큰화\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "\n",
    "print('총 샘플의 개수 : {}'.format(len(result)))\n",
    "print('샘플 3개만 출력')\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-macosx_10_9_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/depark/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/depark/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1 smart-open-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result vector size :  (21613, 100)\n",
      "one word size :  (100,)\n",
      "[('woman', 0.8376989960670471), ('guy', 0.810554563999176), ('lady', 0.783030092716217), ('boy', 0.7661250829696655), ('girl', 0.7360955476760864), ('gentleman', 0.7119325399398804), ('soldier', 0.7082277536392212), ('kid', 0.6837635636329651), ('friend', 0.6609455943107605), ('son', 0.6510811448097229)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "\n",
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "print('result vector size : ', model.wv.vectors.shape)\n",
    "print('one word size : ', model.wv['princess'].shape)\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8376989960670471), ('guy', 0.810554563999176), ('lady', 0.783030092716217), ('boy', 0.7661250829696655), ('girl', 0.7360955476760864), ('gentleman', 0.7119325399398804), ('soldier', 0.7082277536392212), ('kid', 0.6837635636329651), ('friend', 0.6609455943107605), ('son', 0.6510811448097229)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드\n",
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Pre-trained Word2Vec embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# # https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit 다운로드 후 실행\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin 파일 경로', binary=True)  \n",
    "# print('model size : ', model.vectors.shape) # 모델의 크기 확인\n",
    "# print(model.wv.most_similar(\"book\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ***Skip-Gram with Negative Sampling, SGNS***\n",
    "---\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
