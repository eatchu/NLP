{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***RNN Encoder & Decoder***\n",
    "---\n",
    "---\n",
    "---\n",
    "1. *Sequence-to-Sequence, seq2seq*\n",
    "2. *Neural Machine Translation (seq2seq) Tutorial*\n",
    "3. *Bilingual Evaluation Understudy Score*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ***Sequence-to-Sequence, seq2seq***\n",
    "---\n",
    "\n",
    "- 입력 시퀀스로부터 다른 도메인의 시퀀스를 출력\n",
    "- 챗봇, 기계번역, 내용 요약(Text Summarize), STT(Speech to Text)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Sequence-to-Sequence***\n",
    "\n",
    "![](https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG)\n",
    "\n",
    "\n",
    "- **Input(Encoder)** : 단어 토큰화를 통해서 단어 단위로 쪼개진 토큰 각각은 RNN cell의 각 시점으로 입력 됨\n",
    "- **context vector** : 인코더 RNN cell의 마지막 시점의 은닉 상태를 받음\n",
    "    - (N, 1)\n",
    "    - 인코더의 입력 문장 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만든 것\n",
    "    - 과거 시점의 RNN cell에서의 모든 hidden state값들의 영향을 누적해서 받아온 값\n",
    "    - 즉, 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고있음\n",
    "- **Output(Decoder)** : decoder cell의 첫번째 hidden state값으로 context vector를 받아옴\n",
    "    - 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>이 들어감\n",
    "    - 다음에 등장할 확률이 높은 단어를 예측\n",
    "    - 문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
