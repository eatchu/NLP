{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***Text Preprocessing***\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "## ***1. Integer Encoding***\n",
    "---\n",
    "\n",
    "- *각 단어에 고유한 정수를 mapping*\n",
    "- *주로 빈도수가 높은 기준으로 정렬하여 mapping*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### 1. ***Integer Encoding***\n",
    "\n",
    "`method`\n",
    ">1. *Dictionary* <br>\n",
    "2. *Counter (collections)* <br>\n",
    "3. *FreqDist (nltk)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency count :  {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "integer index mapping :  {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n",
      "integer index mapping top 5 :  {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "result =  [[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary 사용하기\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sample text\n",
    "text = \"A barber is a person. a barber is good person. \\\n",
    "a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. \\\n",
    "Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. \\\n",
    "But keeping and keeping such a huge secret to himself was driving the barber crazy. \\\n",
    "the barber went up a huge mountain.\"\n",
    "\n",
    "# sentence tokenize\n",
    "text = sent_tokenize(text)\n",
    "\n",
    "# text cleaning and word tokenize\n",
    "vocab = {} \n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# frequency count\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) \n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "    \n",
    "print('frequency count : ', vocab)\n",
    "\n",
    "\n",
    "# 빈도수가 2이상인 단어를 추출 후 빈도수가 많은 단어순으로 정렬\n",
    "vocab_filter = list(filter(lambda x:x[1]>1, sorted(vocab.items(), key=lambda x:x[1], reverse=True)))\n",
    "\n",
    "# integer index mapping\n",
    "word_index = {}\n",
    "for i in range(len(vocab_filter)):\n",
    "    word_index[vocab_filter[i][0]] = i+1\n",
    "print('integer index mapping : ', word_index)   \n",
    "\n",
    "# 상위 5개 단어만 추출\n",
    "word_index = dict(filter(lambda x:x[1]<=5, word_index.items()))\n",
    "print('integer index mapping top 5 : ', word_index)  \n",
    "\n",
    "# dictionary에 존재하지 않는 단어에 번호를 부여하기 위해 out of vocabulary 정보 입력\n",
    "word_index['OOV'] = len(word_index) + 1\n",
    "\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_index[w])\n",
    "        except:\n",
    "            temp.append(word_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print('result = ', encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence sample 3 :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person']]\n",
      "frequency count :  Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n",
      "frequency count top 5 :  [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "integer index mapping :  {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# Counter 사용하기\n",
    "from collections import Counter\n",
    "print('sentence sample 3 : ',sentences[:3])\n",
    "\n",
    "# words = np.hstack(sentences)\n",
    "words = sum(sentences, [])\n",
    "\n",
    "# frequency count\n",
    "vocab = Counter(words)\n",
    "print('frequency count : ', vocab)\n",
    "\n",
    "# 상위 5개 단어만 추출\n",
    "# word_index = dict(filter(lambda x:x[1]<=5, word_index.items()))\n",
    "vocab = vocab.most_common(5)\n",
    "print('frequency count top 5 : ', vocab)  \n",
    "\n",
    "# integer index mapping\n",
    "word_index = {}\n",
    "for i in range(len(vocab)):\n",
    "    word_index[vocab[i][0]] = i+1\n",
    "print('integer index mapping : ', word_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency count :  <FreqDist with 13 samples and 36 outcomes>\n",
      "frequency count top 5 :  [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
      "integer index mapping :  {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# FreqDist 사용하기\n",
    "from nltk import FreqDist\n",
    "\n",
    "# frequency count\n",
    "vocab = FreqDist(words)\n",
    "print('frequency count : ', vocab)\n",
    "\n",
    "# 상위 5개 단어만 추출\n",
    "vocab = vocab.most_common(5)\n",
    "print('frequency count top 5 : ', vocab)  \n",
    "\n",
    "# integer index mapping\n",
    "word_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print('integer index mapping : ', word_index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. ***Keras integer mapping***\n",
    "\n",
    " - keras tokenizer 함수에는 word count, word index, sequence to index mapping처리를 한번에 할 수 있는 기능이 존재\n",
    " - top n개의 단어만 mapping을 하고 싶을때 파라미터로 지정은 가능하지만 sentence to index mapping시에만 적용됨\n",
    " - dictionary에서도 top n개의 단어만을 남기고 싶다면 조건에 부합하지 않는 key값들을 제거해야함\n",
    " - OOV 단어 보존 시 기존에는 OOV가 마지막 정수로 mapping되었다면 keras에서는 첫번째 정수로 mapping됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence sample 3 :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person']]\n",
      "frequency count :  OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "result =  [[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n",
      "단어 OOV의 인덱스 : 1\n",
      "result =  [[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "print('sentence sample 3 : ',sentences[:3])\n",
    "\n",
    "# frequency count\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print('frequency count : ', tokenizer.word_counts)\n",
    "\n",
    "# 상위 5개 단어만 추출\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# print('frequency count top 5 : ', tokenizer.word_counts)\n",
    "\n",
    "# integer index mapping\n",
    "# print('integer index mapping : ', tokenizer.word_index) \n",
    "print('result = ', tokenizer.texts_to_sequences(sentences))\n",
    "\n",
    "\n",
    "\n",
    "# dictionary에서도 top n개의 단어만을 남기고 싶다면 조건에 부합하지 않는 key값들을 제거해야함\n",
    "'''\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
    "for w in words_frequency:\n",
    "    del tokenizer.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    del tokenizer.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(sentences))\n",
    "'''\n",
    "\n",
    "# Out of Vocabulary 보존하기\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))\n",
    "print('result = ', tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***2. Padding***\n",
    "---\n",
    "\n",
    "- *렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### 1. ***Numpy Padding***\n",
    "> zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence sample 3 :  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person']]\n",
      "max length of sentences =  7\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "print('sentence sample 3 : ',sentences[:3])\n",
    "\n",
    "# index mapping\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max(len(item) for item in encoded)\n",
    "print('max length of sentences = ', max_len)\n",
    "\n",
    "# padding\n",
    "for item in encoded: # 각 문장에 대해서\n",
    "    while len(item) < max_len:   # max_len보다 작으면\n",
    "        item.append(0)\n",
    "        \n",
    "padding_np = np.array(encoded)\n",
    "print(padding_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. ***Keras Padding***\n",
    "> back and forth <br>\n",
    "zero or value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***forth zero padding***\n",
      "[[ 0  0  0  0  0  1  5]\n",
      " [ 0  0  0  0  1  8  5]\n",
      " [ 0  0  0  0  1  3  5]\n",
      " [ 0  0  0  0  0  9  2]\n",
      " [ 0  0  0  2  4  3  2]\n",
      " [ 0  0  0  0  0  3  2]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  6]\n",
      " [ 0  0  0  0  1  4  2]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 0  0  0  1 12  3 13]]\n",
      "***back zero padding***\n",
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n",
      "***back zero padding and maxlen change***\n",
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n",
      "***back value padding***\n",
      "[[ 1  5 14 14 14 14 14]\n",
      " [ 1  8  5 14 14 14 14]\n",
      " [ 1  3  5 14 14 14 14]\n",
      " [ 9  2 14 14 14 14 14]\n",
      " [ 2  4  3  2 14 14 14]\n",
      " [ 3  2 14 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  2 14 14 14 14]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13 14 14 14]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# forth zero padding\n",
    "padded = pad_sequences(encoded)\n",
    "print('***forth zero padding***')\n",
    "print(padded)\n",
    "\n",
    "# back zero padding\n",
    "padded = pad_sequences(encoded, padding = 'post')\n",
    "print('***back zero padding***')\n",
    "print(padded)\n",
    "\n",
    "# back zero padding and maxlen change\n",
    "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)\n",
    "print('***back zero padding and maxlen change***')\n",
    "print(padded)\n",
    "\n",
    "# back value padding\n",
    "last_value = len(tokenizer.word_index) + 1\n",
    "padded = pad_sequences(encoded, padding = 'post', value = last_value)\n",
    "print('***back value padding***')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***3. One Hot Encoding***\n",
    "---\n",
    "\n",
    "\n",
    "> *unique한 단어수가 100개가 있고 실제 출현한 단어수가 200개라면*<br>\n",
    "*원핫인코딩읜 차원은 (200, 100)이됨* <br>\n",
    "*즉 단어 수가 row, 유니크한 단어 수가 col(벡터의 차원)이 됨*\n",
    "\n",
    "`method`\n",
    "1. *Index integer mapping*\n",
    "2. *One Hot Encoding*\n",
    "\n",
    "\n",
    "`one hot encoding의 한계`\n",
    " - 단어 수가 많아질수록 벡터의 차원이 계속 늘어나 저장 공간 측면에서 매우 비효율적\n",
    " - 단어의 유사도를 표현하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "\n",
    "# 형태소 분석기를 통한 tokenize\n",
    "okt=Okt()  \n",
    "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
    "print(token)\n",
    "\n",
    "word2index={}\n",
    "for voca in token:\n",
    "     if voca not in word2index.keys():\n",
    "       word2index[voca]=len(word2index)\n",
    "print(word2index)\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "   one_hot_vector = [0]*(len(word2index))\n",
    "   index=word2index[word]\n",
    "   one_hot_vector[index]=1\n",
    "   return one_hot_vector\n",
    "\n",
    "one_hot_encoding(\"자연어\",word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# keras 사용하기\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# word index create\n",
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# sentence to int\n",
    "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded=tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)\n",
    "\n",
    "# one hot encoding\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***4. Splitting Data***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Supervised Learning***\n",
    "\n",
    "> *sample dataset*\n",
    "\n",
    "텍스트(메일의 내용)|레이블(스팸 여부)\n",
    "---|---\n",
    "당신에게 드리는 마지막 혜택! ...|스팸 메일\n",
    "내일 뵐 수 있을지 확인 부탁...|정상 메일\n",
    "...|...\n",
    "(광고) 멋있어질 수 있는...|스팸 메일\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. ***Split X, y***\n",
    "1. zip function\n",
    "2. DataFrame\n",
    "3. Numpy\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***Split test data***\n",
    " - train_test_split (sklearn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***5. Text Preprocessing Tools for Korean Text***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***PyKoSpacing***\n",
    " - 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지\n",
    " \n",
    " \n",
    "<br>\n",
    "\n",
    "#### 2. ***Py-Hanspell***\n",
    " - 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "# !pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pykospacing import Spacing\n",
    "\n",
    "# sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. \\\n",
    "# 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'\n",
    "\n",
    "# new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "# print(new_sent)\n",
    "\n",
    "# kospacing_sent = Spacing(new_sent)\n",
    "# print(sent)\n",
    "# print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n",
      "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "\n",
    "# 맞춤법 검사\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)\n",
    "      \n",
    "# 뛰어쓰기\n",
    "spelled_sent = spell_checker.check(new_sent)\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3. ***SOYNLP***\n",
    " - 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    " - 비지도 학습으로 단어 토큰화를 하며 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    " - 내부적으로 단어 점수 표로 동작하며 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
