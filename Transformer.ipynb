{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***Transformer***\n",
    "---\n",
    "---\n",
    "---\n",
    "1. *Transformer Algorithm*\n",
    "2. *Transformer Chatbot Tutorial*\n",
    "3. *Multi-head Self Attention for Text Classification*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ***Transformer Algorithm***\n",
    "---\n",
    "\n",
    "- Attention을 RNN의 보정을 위한 용도가 아닌 실제로 인코더와 디코더를 만드는 방법론\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Hyperparameter***\n",
    "\n",
    "> $d_{model} = 512$ : *트랜스포머의 인코더와 디코더에서 정해진 입력과 출력의 크기. 즉, 임베딩 벡터의 차원을 말함.*\n",
    "\n",
    "> $num_ \\,layers = 6$ : *트렌스포머 모델에서 인코더와 디코더의 레이어 층 구성을 의미. 각 인코더와 디코더의 레이어 수.*\n",
    "\n",
    "> $num_ \\,heads = 8$ : *어텐션 사용 시 여러 개로 분할하여 병렬로 어텐션을 수행. 이때 이 병렬의 수.*\n",
    "\n",
    "> $d_{ff} = 2048$ : *트렌스포머 내부에 존재하는 신경망의 은닉층의 크기.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. ***Mechanism***\n",
    "\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/31379/transformer2.PNG) <br>\n",
    " - 기존의 seq2seq 구조에서 t개의 시점을 가진 encoder와 decoder가 각각 하나씩 존재하는 구조였다면 트랜스포머에서는 encoder와 decoder 단위가 N개로 구성됨\n",
    "\n",
    " \n",
    "<br> \n",
    " \n",
    "#### **`Positional Encoding`**\n",
    "\n",
    "![](https://wikidocs.net/images/page/31379/transformer6_final.PNG) <br>\n",
    "- 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아님\n",
    "- 따라서 단어의 위치 정보를 알려줄 필요가 있음\n",
    "- 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용 (=포지셔널 인코딩)\n",
    "\n",
    "> **transformer의 위치 정보를 가진 값을 만들기 위해 사용되는 함수** <br> \n",
    "$PE_{(pos, 2_i)} = sin(pos / 10000^{2_i/d_{model}})$ <br>\n",
    "$PE_{(pos, 2_{i+1})} = cos(pos / 10000^{2_i/d_{model}})$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
