{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***Recurrent Neural Network, RNN***\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "1. *Recurrent Neural Network, RNN*\n",
    "2. *Long Short-Term Memory, LSTM*\n",
    "3. *Gated Recurrent Unit, GRU*\n",
    "4. *SimpleRNN과 LSTM*\n",
    "5. *Recurrent Neural Network Language Model, RNNLM*\n",
    "6. *Text Generation using RNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ***Recurrent Neural Network, RNN***\n",
    "---\n",
    "\n",
    " - RNN은 대표적인 sequence model\n",
    " - DNN은 Feed Forward Neural Network 구조지만 RNN의 경우 Recurrent Neural Network 구조를 가짐\n",
    " - 즉, 출력층으로만 향하는 기존의 신경망과 달리 RNN은 출력층의 값이 다시 은닉층 노드로 향하는 순환신경망 구조\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***1. RNN***\n",
    "\n",
    " - RNN에서의 cell이란 ?\n",
    "     - 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드\n",
    "     - 이 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 메모리 셀 또는 RNN 셀이라고 표현\n",
    "     - 은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있음\n",
    "     - 메모리 셀이 출력층 방향으로 또는 다음 시점 t+1의 자신에게 보내는 값을 은닉 상태(hidden state)라고 함\n",
    "\n",
    "> RNN Structure <br>\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG) <br>\n",
    "\n",
    "> Detailed Structure <br>\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image2.5.PNG) <br>\n",
    "\n",
    "> RNN Equation <br>\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG)<br>\n",
    "은닉층 : $h_t = tanh(w_x x_t + w_h h_{t-1} + b)$ <br>\n",
    "출력층 : $y_t = f(w_y h_t + b)$\n",
    "\n",
    "> RNN Input Shape <br>\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image6between7.PNG)\n",
    "\n",
    "- How to print output state from cell?\n",
    "    1. 메모리 셀의 최종 시점의 은닉 상태만을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서를 리턴\n",
    "    2. 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 전체 시퀀스를 리턴하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴\n",
    "        - using `return_sequences = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.87137823 0.96849597 0.92462722 0.98108858 0.85341294 0.91668457\n",
      "  0.96730208 0.79581152]\n",
      " [0.99970633 0.99991227 0.99996763 0.99950346 0.99974216 0.99925189\n",
      "  0.99972503 0.99976492]\n",
      " [0.99985349 0.99994016 0.99998157 0.99977212 0.99983561 0.99965979\n",
      "  0.99988174 0.99991575]\n",
      " [0.99993806 0.99998953 0.99999255 0.9999555  0.99997966 0.99990359\n",
      "  0.99998081 0.99995623]\n",
      " [0.99992786 0.99998231 0.99999437 0.99992596 0.99997286 0.99984459\n",
      "  0.99997404 0.9999584 ]\n",
      " [0.99994168 0.99999403 0.99999624 0.99993958 0.99996639 0.99987995\n",
      "  0.99996377 0.99995486]\n",
      " [0.99992397 0.99998967 0.99999375 0.99987852 0.99997787 0.99982952\n",
      "  0.99992822 0.9999152 ]\n",
      " [0.99990211 0.99996941 0.99999221 0.99985981 0.99994747 0.99975539\n",
      "  0.99994473 0.99994252]\n",
      " [0.99995598 0.99999541 0.99999712 0.99996143 0.99999211 0.99991691\n",
      "  0.99998413 0.99996154]\n",
      " [0.99989453 0.99997007 0.99998958 0.99985916 0.99992035 0.99976239\n",
      "  0.99993181 0.99993547]]\n"
     ]
    }
   ],
   "source": [
    "# 함수 사용 없이 의사코드로 RNN 구현하기\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_dim = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim)) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬.\n",
    "# print(hidden_state_t) \n",
    "\n",
    "Wx = np.random.random((hidden_size, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
    "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "  hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### ***2. Bidirectional Recurrent Neural Network***\n",
    "\n",
    " - 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터에도 영향을 받을 수 있는 관점이다.\n",
    " - 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에서도 힌트를 찾는다.\n",
    " - 양방향 RNN은 기본적으로 두개의 메모리 셀을 사용한다.\n",
    "     1. 앞 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산하는 셀. (Forward States)\n",
    "     2. 뒤 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산하는 셀. (Backward States)\n",
    "     \n",
    "     \n",
    "![](https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG)\n",
    "*주황색 cell이 Forward States를 받으며 초록색 cell이 Backward States를 받는다*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 2, 256)            35584     \n",
      "=================================================================\n",
      "Total params: 35,584\n",
      "Trainable params: 35,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 2, 256)            35584     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2, 256)            98560     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 2, 256)            98560     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 2, 256)            98560     \n",
      "=================================================================\n",
      "Total params: 331,264\n",
      "Trainable params: 331,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Bidirectional\n",
    "\n",
    "timesteps = 2\n",
    "input_dim = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))\n",
    "model.summary()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> EXAMPLE\n",
    "\n",
    "1. Embedding을 사용하며, 단어 집합(Vocabulary)의 크기가 5,000이고 임베딩 벡터의 차원은 100입니다.\n",
    "2. 은닉층에서는 Simple RNN을 사용하며, 은닉 상태의 크기는 128입니다.\n",
    "3. 훈련에 사용하는 모든 샘플의 길이는 30으로 가정합니다.\n",
    "4. 이진 분류를 수행하는 모델로, 출력층의 뉴런은 1개로 시그모이드 함수를 사용합니다.\n",
    "5. 은닉층은 1개입니다.\n",
    "\n",
    "\n",
    "> RESULT\n",
    "\n",
    " - embedding = 5000 x 100 = 500000\n",
    " - wx = 100 x 128 = 12800\n",
    " - wh = 128 x 128 = 16384\n",
    " - h = 128\n",
    " - wy = 128\n",
    " - output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               29312     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 529,441\n",
      "Trainable params: 529,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(SimpleRNN(hidden_size))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ***Long Short-Term Memory, LSTM***\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***1. Vanila RNN***\n",
    " \n",
    "![](https://wikidocs.net/images/page/22888/lstm_image1_ver2.PNG) <br>\n",
    "- vanila RNN은 비교적 짧은 시퀀스에 대해서만 효과를 보인다.\n",
    "- timestep이 길어질 수록 앞의 정보가 충분히 전달되지 못하는 현상이 발생한다.\n",
    "- 장기 의존성 문제 (The Problem of Long-Term Dependencies)를 가진다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***2. LSTM***\n",
    "\n",
    "> RNN과 LSTM의 내부 구조 비교\n",
    "\n",
    "![](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG) \n",
    "![](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG) <br>\n",
    "\n",
    "- 기존 은닉층의 메모리 셀에 **입력게이터, 망각게이트, 출력게이트**를 추가\n",
    "- 불필요한 기억을 지우고, 기억해야할 것들을 정함\n",
    "- hidden state와 cell state를 전달하게됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ***Gated Recurrent Unit, GRU***\n",
    "---\n",
    "\n",
    "- LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄임\n",
    "- 즉, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22889/GRU.PNG) <br>\n",
    " - 기존 LSTM은 입력, 망각, 출력 게이트가 존재\n",
    " - GRU에서는 업데이트, 리셋 두개의 게이트를 사용\n",
    " \n",
    "<br>\n",
    " \n",
    ">실제 GRU 은닉층을 추가하는 코드. <br>\n",
    "model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ***SimpleRNN과 LSTM***\n",
    "---\n",
    "\n",
    "> **example** <br>\n",
    "dim = 5 <br>\n",
    "timestep(length) = 4 <br>\n",
    "batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install wrapt\n",
    "import numpy as np    \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional\n",
    "\n",
    "# dim = 5, length(ts) = 4, bs = 1\n",
    "train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\n",
    "train_X = np.array(train_X, dtype=np.float32)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### ***1. Simple RNN 구현***\n",
    "\n",
    "1. SimpleRNN(default)\n",
    "2. SimpleRNN(retrun_sequences=True)\n",
    "3. SimpleRNN(return_sequences=True, return_state=True)\n",
    "    - output1 : hidden state at all timestep\n",
    "    - output2 : hidden state at last timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[ 0.94330335 -0.97819847 -0.8401196 ]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. SimpleRNN(default)\n",
    "rnn = SimpleRNN(3)\n",
    "# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일.\n",
    "hidden_state = rnn(train_X)\n",
    "\n",
    "# 최종  output : 마지막 timestep에서의 출력값\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.9938013  -0.82615274 -0.47604564]\n",
      "  [-0.99873894 -0.66273797 -0.1694383 ]\n",
      "  [-0.99903196  0.8662755  -0.983821  ]\n",
      "  [-0.03076643 -0.9436481  -0.8706647 ]]], shape: (1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. SimpleRNN(retrun_sequences=True)\n",
    "rnn = SimpleRNN(3, return_sequences=True)\n",
    "hidden_states = rnn(train_X)\n",
    "\n",
    "# 최종 output : 모든 timestep에서의 출력값\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.72086656  0.9934091  -0.953686  ]\n",
      "  [ 0.19770178  0.8997607  -0.976504  ]\n",
      "  [ 0.42180285  0.9290261  -0.09734499]\n",
      "  [-0.9015882   0.20464553 -0.6407577 ]]], shape: (1, 4, 3)\n",
      "last hidden state : [[-0.9015882   0.20464553 -0.6407577 ]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3. SimpleRNN(return_sequences=True, return_state=True)\n",
    "rnn = SimpleRNN(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_state = rnn(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### ***2. LSTM 구현***\n",
    "1. LSTM(default)\n",
    "2. LSTM(retrun_sequences=True)\n",
    "3. LSTM(return_sequences=True, return_state=True)\n",
    "    - output1 : hidden state at all timestep\n",
    "    - output2 : hidden state at last timestep\n",
    "    - output3 : cell state at last timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[ 0.07909779 -0.05342373 -0.03087045]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. LSTM(default)\n",
    "lstm = LSTM(3)\n",
    "hidden_state = lstm(train_X)\n",
    "\n",
    "# 최종  output : 마지막 timestep에서의 출력값\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.04998615 -0.4816332  -0.09267705]\n",
      "  [-0.0082723  -0.705354   -0.3168589 ]\n",
      "  [ 0.29138252 -0.50929123 -0.31556296]\n",
      "  [ 0.08178879 -0.4883236  -0.10235637]]], shape: (1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2. LSTM(retrun_sequences=True)\n",
    "lstm = LSTM(3, return_sequences=True)\n",
    "hidden_states = lstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.05346462 -0.14736414 -0.55206156]\n",
      "  [-0.08086106 -0.24957754 -0.5361125 ]\n",
      "  [-0.1453886  -0.24032782 -0.25564075]\n",
      "  [ 0.00431521 -0.27720132 -0.23613769]]], shape: (1, 4, 3)\n",
      "last hidden state : [[ 0.00431521 -0.27720132 -0.23613769]], shape: (1, 3)\n",
      "last cell state : [[ 0.00641178 -0.4084831  -0.41746327]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3. LSTM(retrun_sequences=True, return_state=True)\n",
    "lstm = LSTM(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\n",
    "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "#### ***3. Bidirectional(LSTM) 구현***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_init = tf.keras.initializers.Constant(value=0.1)\n",
    "b_init = tf.keras.initializers.Constant(value=0)\n",
    "r_init = tf.keras.initializers.Constant(value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[0.6303138 0.6303138 0.6303138 0.7038734 0.7038734 0.7038734]], shape: (1, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. Bidirectional(retrun_sequences=False, return_state=True)\n",
    "bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[0.35906473 0.35906473 0.35906473 0.7038734  0.7038734  0.7038734 ]\n",
      "  [0.55111325 0.55111325 0.55111325 0.58863586 0.58863586 0.58863586]\n",
      "  [0.59115744 0.59115744 0.59115744 0.3951699  0.3951699  0.3951699 ]\n",
      "  [0.6303138  0.6303138  0.6303138  0.21942244 0.21942244 0.21942244]]], shape: (1, 4, 6)\n",
      "forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n",
      "backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. Bidirectional(retrun_sequences=True, return_state=True)\n",
    "bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
