{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# *Text Preprocessing*\n",
    "---\n",
    "---\n",
    "---\n",
    "***Text Preprocessing1***\n",
    "1. *Tokenization*\n",
    "2. *Cleaning and Nomalization*\n",
    "3. *Stemming and Lemmatization*\n",
    "4. *Stopword*  \n",
    "5. *Regular Expression*\n",
    "***Text Preprocessing2*** \n",
    "1. *Integer Encoding*\n",
    "2. *Padding*\n",
    "3. *One-Hot-Encoding*\n",
    "4. *Splitting Data*\n",
    "5. *Text Preprocessing Tools for Koreans Text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***1. Tokenization***\n",
    "---\n",
    " - *자연어처리에서 크롤링 등으로 얻어낸 **코퍼스(corpus)** 데이터를 토큰(token)이라 불리는 단위로 작업이며 의미있는 단위로 토큰을 정의*\n",
    " \n",
    "     - ***corpus(말뭉치)***\n",
    "        - 자연어처리를 위한 문장들로 구성된 데이터셋\n",
    "        - 자연어처리 연구를 염두에 두고 수집된 대량의 텍스트 데이터\n",
    "        - parallel corpus : I love to go to school == 나는 학교에 가는 것을 좋아한다 -> labeling\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "#### 1. ***단어 토큰화***\n",
    " - *구두점(punctuation)과 같은 문자를 제외 시키는 단어 토큰화* \n",
    ">*입력 : Time is an illusion. Lunchtime double so!* <br>\n",
    "*출력 : \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\"*\n",
    " - *하지만 이런 토큰화 과정 중에서는 예상치 못한 문제점이 존재*\n",
    ">*입력 : **Don't** be fooled by the dark sounding name, Mr. **Jone's** Orphanage is as cheery as cheery goes for a pastry shop.* <br>\n",
    "*출력 : ???* <br> <br>\n",
    "*How to tokenization with **Don't** and **Jone's** word?* <br>\n",
    "*1. ***word_tokenize***(nltk) :* ***[Do / n't]*** *,* ***[Jone / 's]***<br>\n",
    "*2. ***WordPunctTokenizer***(nltk) : **[Don / ' / t]**, **[Jone / ' / s]** => 구두점을 별도로 분리*<br>\n",
    "*3. ***text_to_word_sequence***(tensorflow) : **[Don't]**, **[Jone's]** => 구두점을 제거하되 아포스트로피는 보존*<br>\n",
    "***=> 코드를 통해 알아보자***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - *단어 토큰화 시 고려사항*\n",
    " > *1) 구두점이나 특수문자를 제외 할 경우 : **.**(문장의 마침을 의미) / **M&M**(구두점을 포함하는 단어) / **$-** (가격이나 날짜를 의미하는 단어) 등*<br>\n",
    "*2) 줄임말과 단어 내에 뛰어쓰기가 있는 경우 : we're -> we are / New York (뛰어쓰기가 포함되어야 하는 단어)*\n",
    "\n",
    " - *표준 토큰화 예제 : ***TreebankWordTokenizer***(nltk)*\n",
    "     - *하이푼(-)으로 구성된 단어는 하나로 유지*\n",
    "     - *아포스트로피로 접어가 함께하는 단어는 분리*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. ***문장 토큰화(Sentence Tokenization)***\n",
    "\n",
    " - *sentence segmentation은 어떻게 해야할까?*\n",
    "> *문장 토큰화는 직관적으로 생각했을때 마침표나 느낌표 기준으로 문장을 자르면 된다고 생각할 수 있다.*<br>\n",
    "*하지만 마침표는 문장의 끝이 아니더라도 가지고 있을 수 있으며 어떤 언어를 사용하는지 또눈 오타나 다양한 특수문자 사용에 의해 분리가 어려울 수 있다.*\n",
    "\n",
    " - *문장 토큰화 실습*\n",
    "     - *English : ***sent_tokenize***(nltk)*\n",
    "     - *Korea : ***split_sentences***(kss)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3. ***한국어 토큰화의 어려움***\n",
    "\n",
    "1. *교착어*\n",
    " - *한국어는 어절이 독립적으로 구성되어 있는 것이 아니라 조사 등 무언가가 붙은채로 의미를 가지는 경우가 많음*\n",
    "     - *자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.*\n",
    "     - *의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다.*\n",
    " - *따라서 한국어는 어절 토큰화보다 형태소 토큰화를 수행하는 것이 좋음*\n",
    " \n",
    "2. *띄어쓰기*\n",
    " - *많은 경우에 띄어쓰기가 틀리거나 잘 지켜지지 않는 경우가 많음 (영어와 달리 띄어쓰기가 잘 수행되지 않아도 의미 파악이 가능)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4. ***품사 태깅 : Part-of-speech tagging***\n",
    "\n",
    " - *English*\n",
    "     - ***pos_tag*** *(nltk)*\n",
    " - *Korea : 형태소 분석기 자체에서 품사태깅 진행*\n",
    "     - ***Okt*** *(Open Korea Text), Mecab, Komoran, Hannanum, ***Kkma*** : konlpy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "x = pos_tag(word_tokenize(text))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d160af2c26d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mokt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moktJavaPackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kr.lucypark.okt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/konlpy/jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[0;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclasspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mjvmpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjvmpath\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDefaultJVMPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/jpype/_jvmfinder.py\u001b[0m in \u001b[0;36mgetDefaultJVMPath\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinuxJVMFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_jvm_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/jpype/_jvmfinder.py\u001b[0m in \u001b[0;36mget_jvm_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjvm_notsupport_ext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mjvm_notsupport_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         raise JVMNotFoundException(\"No JVM shared library file ({0}) \"\n\u001b[0m\u001b[1;32m    213\u001b[0m                                    \u001b[0;34m\"found. Try setting up the JAVA_HOME \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    \u001b[0;34m\"environment variable properly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJVMNotFoundException\u001b[0m: No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "okt=Okt()  \n",
    "print(okt.morphs(text))\n",
    "print(okt.pos(text))  \n",
    "print(okt.nouns(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma  \n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "kkma=Kkma()  \n",
    "print(kkma.morphs(text))\n",
    "print(kkma.pos(text))  \n",
    "print(kkma.nouns(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***2. Cleaning and Nomalization***\n",
    "---\n",
    " - ***정제(cleaning)*** : *갖고 있는 코퍼스로부터 노이즈 데이터를 제거.*\n",
    "      -  토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만 <br> 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 함\n",
    " - ***정규화(normalization)*** : *표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.*\n",
    " \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### 1. ***단어들의 통합***\n",
    " - ***하나의 단어로 통합*** : *단어는 다르지만 의미가 같은 경우*\n",
    " - ***대소문자 통합*** : *대문자 -> 소문자*\n",
    "     - *대문자가 의미가 있는 경우도 있기때문에 문장의 맨 앞에 나오는 대문자만 소문자로 바꾸는 것도 고려가능*\n",
    "     - *일일히 예외사항을 고려하는 것이 더 어려울 수 있으므로 어느정도의 예외사항을 배제하고 모두 소문자로 바꿔 사용하는 것이 실용적인 해결책이 될 수 있음*\n",
    "     \n",
    "<br>\n",
    "\n",
    "#### 2. ***불필요한 단어의 제거(Removing Unnecessary Words)***\n",
    " - *노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만 <br> 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 함.*\n",
    " \n",
    "1. ***등장 빈도가 적은 단어(Removing Rare words)***\n",
    " - N개의 메일 데이터에서 총 합 1~5번 밖에 등장하지 않은 단어가 있다면 이 단어는 직관적으로 분류에 거의 도움이 되지 않을 것.\n",
    "2. ***길이가 짧은 단어(Removing words with very a short length)***\n",
    " - 영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과를 볼 수 있음\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***3. Stemming and Lemmatization***\n",
    "---\n",
    ">표제어 추출과 어간 추출의 다른점 알아보기\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "#### 1. ***표제어 추출(Lemmatization)***\n",
    " - *표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가 하나의 단어로 통합하는 것*\n",
    "     - ex) am, are, is -> be\n",
    " - *표제어 추출을 하는 가장 섬세한 방법은 단어의 ***형태학적 파싱***을 먼저 진행하는 것*\n",
    " - *형태학적 파싱이란 형태소의 어간과 접사를 분리하는 것*\n",
    "     - ex) cats -> cat / s\n",
    " - ***즉 단어를 뿌리 단어의 형태로 바꾸는 것***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 : \n",
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
      "\n",
      "\n",
      "잘못 추출된 단어들에 대해 품사를 지정하여 다시 표제어 추출해보기 :\n",
      "['die', 'watch', 'have']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('표제어 추출 : ')\n",
    "print([n.lemmatize(w) for w in words])\n",
    "print('\\n')\n",
    "print('잘못 추출된 단어들에 대해 품사를 지정하여 다시 표제어 추출해보기 :')\n",
    "print([n.lemmatize('dies', 'v'), n.lemmatize('watched', 'v'), n.lemmatize('has', 'v')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2. ***어간 추출(Stemming)***\n",
    "\n",
    " - ***PorterStemmer***\n",
    "     - 규칙 : ALIZE → AL / ANCE → 제거 / ICAL → IC\n",
    "         - ex) formalize → formal / allowance → allow / electricical → electric\n",
    "     -   Porter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인 가능\n",
    " - ***LancasterStemmer***\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer VS LancasterStemmer\n",
    "\n",
    "s=PorterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3. ***한국어에서의 어간 추출***\n",
    "\n",
    "언 | 품사 |\n",
    "-- | -- |\n",
    "체언 | 명사, 대명사, 수사\n",
    "수식언 | 관형사, 부사\n",
    "관계언 | 조사\n",
    "독립언 | 감탄사\n",
    "용언 | 동사, 형용사\n",
    "\n",
    "1. *활용(conjugation)*\n",
    " - *어간(stem) + 어미(ending)*\n",
    "     - 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\n",
    "     - 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행\n",
    "     \n",
    "2. *규칙 활용*\n",
    "3. *불규칙 활용*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***4. Stopword***\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "#### 1. ***NLTK에서 불용어 확인하기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "#### 2. ***NLTK를 통해서 불용어 제거하기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "#### 3. ***한국어에서 불용어 제거하기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ***5. Regular Expression***\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "#### 1. ***정규 표현식 문법과 모듈 함수***\n",
    "\n",
    "> ***특수 문자***\n",
    "\n",
    "특수 문자|설명\n",
    "---|---\n",
    ".|한 개의 임의의 문자를 나타냅니다. (줄바꿈 문자인 \\n는 제외)\n",
    "?|앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 또는 1개)\n",
    "*|앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 이상)\n",
    "+|앞의 문자가 최소 한 개 이상 존재합니다. (문자가 1개 이상)\n",
    "^|뒤의 문자로 문자열이 시작됩니다.\n",
    "$|앞의 문자로 문자열이 끝납니다.\n",
    "{숫자}|숫자만큼 반복합니다.\n",
    "{숫자1, 숫자2}|숫자1 이상 숫자2 이하만큼 반복합니다. ?, *, +를 이것으로 대체할 수 있습니다.\n",
    "{숫자,}|숫자 이상만큼 반복합니다.\n",
    "[ ]|대괄호 안의 문자들 중 한 개의 문자와 매치합니다.<br> [amk]라고 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미합니다. <br> [a-z]와 같이 범위를 지정할 수도 있습니다. <br> [a-zA-Z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미합니다.\n",
    "[^문자]|해당 문자를 제외한 문자를 매치합니다.\n",
    "l|AlB와 같이 쓰이며 A 또는 B의 의미를 가집니다.\n",
    "\n",
    "> ***문자 규칙***\n",
    "\n",
    "문자 규칙|설명\n",
    "---|---\n",
    "\\\\|역 슬래쉬 문자 자체를 의미합니다\n",
    "\\d|모든 숫자를 의미합니다. [0-9]와 의미가 동일합니다.\n",
    "\\D|숫자를 제외한 모든 문자를 의미합니다. [^0-9]와 의미가 동일합니다.\n",
    "\\s|공백을 의미합니다. [ \\t\\n\\r\\f\\v]와 의미가 동일합니다.\n",
    "\\S|공백을 제외한 문자를 의미합니다. [^ \\t\\n\\r\\f\\v]와 의미가 동일합니다.\n",
    "\\w|문자 또는 숫자를 의미합니다. [a-zA-Z0-9]와 의미가 동일합니다.\n",
    "\\W|문자 또는 숫자가 아닌 문자를 의미합니다. [^a-zA-Z0-9]와 의미가 동일합니다.\n",
    "\n",
    "\n",
    "> ***모듈 함수***\n",
    "\n",
    "\n",
    "모듈 함수|설명\n",
    "---|---\n",
    "re.compile()|정규표현식을 컴파일하는 함수입니다. 다시 말해, 파이썬에게 전해주는 역할을 합니다. <br> 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리합니다.\n",
    "re.search()|문자열 전체에 대해서 정규표현식과 매치되는지를 검색합니다.\n",
    "re.match()|문자열의 처음이 정규표현식과 매치되는지를 검색합니다.\n",
    "re.split()|정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴합니다.\n",
    "re.findall()|문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴합니다. <br> 만약, 매치되는 문자열이 없다면 빈 리스트가 리턴됩니다.\n",
    "re.finditer()|문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴합니다.\n",
    "re.sub()|문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
