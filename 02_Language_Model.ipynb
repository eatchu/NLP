{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# ***LANGUAGE MODEL***\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "1. *What is Language Model?*\n",
    "2. *Statistical Language Model, SLM*\n",
    "3. *N-gram Language Model*\n",
    "4. *Language Model for Korean Sentences*\n",
    "5. *Perplexity*\n",
    "6. *Conditional Probability*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ***What is Language Model?***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Language Model***\n",
    "\n",
    " - 단어 시퀀스에 확률을 할당하는 모델. 즉, 가장 자연스러운 단어 시퀀스를 찾아내는 모델\n",
    " - 이전 단어들이 주어졌을 때 다음 단어를 예측\n",
    " \n",
    "<br>\n",
    "\n",
    "#### 2. ***Probability Assign***\n",
    "\n",
    "- 확률을 통해 보다 적절한 문장을 판단\n",
    "\n",
    "1) Machine Translation : $P(나는\\;버스를\\;탔다) > P(나는\\;버스를\\;태운다)$ <br>\n",
    "2) Spell Correction : 선생님이 교실로 부리나케 $P(달려갔다) > P(잘려갔다)$ <br>\n",
    "3) Speech Recognition : $P(나는\\;메롱을\\;먹는다) < P(나는\\;메론을\\;먹는다)$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***Word Prediction***\n",
    "\n",
    ">  w = 단어, W = 시퀀스, n = 단어 수\n",
    "1) 단어 시퀀스의 확률 : $P(W) = P(w_1, w_2, w_3, ..., w_n)$ <br>\n",
    "2) 다음 단어 등장 확률 : $P(W) = \\prod_{i=1}^n P(w_n|w_1,...,w_{n-1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ***Statistical Language Model, SLM***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Conditional Probability***\n",
    "*Conditional Probability ->* $P(A,B) = P(A)P(B|A)$ <br>\n",
    "*Chian Rule of 4 Probability ->*$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$ <br>\n",
    "*Chian Rule of n Probability ->*$P(x_1,x_2...x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,x_2,...,x_{n-1})$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. ***Sentence Probability***\n",
    "*TEXT = \"An adorable little boy is spreading smiles\"* <br>\n",
    "$P(An\\,adorable\\;little\\;boy\\;is\\;spreading\\;smiles) =$ <br>\n",
    "$P(An) * P(adorable|An) * P(little|An\\;adorable) * P(boy|An\\;adorable\\;little) * P(is|An\\;adorable\\;little\\;boy)$ <br>\n",
    "$* P(spreading|An\\;adorable\\;little\\;boy\\;is) * P(smiles|An\\;adorable\\;little\\;boy\\;is\\;spreading)$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***Count-based Approach***\n",
    "$P(is|An\\;adorable\\;little\\;boy) = \\frac{count(An\\;adorable\\;little\\;boy\\;is)}{count(An\\;adorable\\;little\\;boy)}$ <br>\n",
    "> \"An adorable little boy\" 문장이 100번 등장 <br>\n",
    "\"An adorable little boy is\" 문장이 30번 등장했다면 <br>\n",
    "$P(is|An\\;adorable\\;little\\;boy)$ 확률은 30%\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 4. ***Sparsity Problem***\n",
    "- 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제\n",
    "- SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점\n",
    "- 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ***N-gram Language Model***\n",
    "---\n",
    "\n",
    "- n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종\n",
    "- 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용\n",
    "- 일부 단어를 몇 개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***코퍼스에서 카운트하지 못하는 경우의 감소***\n",
    "$P(is|An\\;adorable\\;little\\;boy) \\approx P(is|boy)$ <br>\n",
    "> An adorable little boy가 나왔을 때 is가 나올 확률에 비해 boy가 나왔을때 is가 나올 확률을 고려 <br>\n",
    " \n",
    "$P(is|An\\;adorable\\;little\\;boy) \\approx P(is|little \\; boy)$ <br>\n",
    "> 위의 사례가 지나친 일반화로 느껴진다면 앞 단어의 한두개 정도를 더 포함해도 좋음\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. ***N-gram***\n",
    " - 임의의 개수를 정하기 위한 기준을 위해 사용하는 것 \n",
    " \n",
    "*TEXT = \"An adorable little boy is spreading smiles\"* <br>\n",
    "> **unigrams** : an, adorable, little, boy, is, spreading, smiles <br>\n",
    "**bigrams** : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles <br>\n",
    "**trigrams** : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles <br>\n",
    "**4-grams** : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
    " \n",
    "- n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존\n",
    "*TEXT = \"An adorable little boy is spreading\"* <br>\n",
    "> n=4라고 한 4-gram을 이용한 언어 모델을 사용한다면 n-1에 해당되는 앞 3개의 단어만 고려됨. <br>\n",
    "$P(w|boy\\;is\\;spreading) = \\frac{count(boy\\;is\\;spreading\\;w)}{count(boy\\;is\\;spreading)}$ <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***N-gram의 한계***\n",
    " - 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재\n",
    " - 전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐\n",
    " - 결국 n을 선택하는 것은 trade-off 문제\n",
    "     1. n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각. (모델 사이즈도 커짐)\n",
    "     2. 을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐\n",
    "     \n",
    "> ***n은 최대 5를 넘게 잡아서는 안 된다고 권장됨***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ***Language Model for Korean Sentences***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***한국어는 어순이 중요하지 않다***\n",
    "\n",
    " - 한국어는 영어와 비교해 어순이 중요하지 않기 때문에 확률에 기반한 언어 모델이 다음 단어를 예측하기 어려움\n",
    "\n",
    "Ex) <br>\n",
    "① 나는 운동을 합니다 체육관에서. <br>\n",
    "② 나는 체육관에서 운동을 합니다. <br>\n",
    "③ 체육관에서 운동을 합니다. <br>\n",
    "④ 나는 운동을 체육관에서 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. ***한국어는 교착어이다***\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3. ***한국어는 띄어쓰기가 제대로 지켜지지 않는다***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ***Perplexity***\n",
    "---\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. ***Evaluation Metric : PPL***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
